{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vaibhav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'herself', 'been', 'with', 'here', 'very', 'doesn', 'won']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[0:500:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIRSW WE ARE OPEINING OUR DATA AND READING IT SO SEE WHAT IT LOOKS LIKE\n",
    "# WE REALIZE THAT THE VALUES IN A ROW ARE SEPARATED BY 'TAB' SPACE AND 'NEW LINES'\n",
    "\n",
    "data = open(\"SMSSpamCollection.tsv\").read()\n",
    "\n",
    "# WE ARE PRINTING THE FIRST 500 TEXT OF OUR DATA\n",
    "\n",
    "data[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE BELOW CODE WE HAVE REPLACE THE 'TAB' SPACE WITH 'NEW LINE' \n",
    "# AND THEN WE USE THE SPLIT METHOD TO MAKE TO LIST, WHENEVER IT SEE THE '\\n' IT WILL CHOP IT OFF \n",
    "# ...and ADD THAT COMPONENT TO THE LIST\n",
    "\n",
    "parseData = data.replace('\\t', '\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n",
       " 'spam',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'ham']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOW WE CAN SEE BELOW THAT OUR DATA IS CONVERTED INTO LIST\n",
    "\n",
    "parseData[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW WE ARE CREATING THE NEW LIST AND PULLING EVERY SECOND ITEM TO IT\n",
    "# ....MEANS WE ARE TAKING THE EVEN VALUES THAT ARE 'HAM' AND 'SPAM'\n",
    "\n",
    "# AND WE HAVE CREATE ANOTHER LIST IN WHICH WE ARE PULLING THE TEXT, WHICH ARE AT THE 'ODD' POSITIONS\n",
    "labelList = parseData[0::2]\n",
    "textList = parseData[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam', 'ham', 'ham', 'ham'] \n",
      "\n",
      "[\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\", \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", \"Nah I don't think he goes to usf, he lives around here though\", 'Even my brother is not like to speak with me. They treat me like aids patent.', 'I HAVE A DATE ON SUNDAY WITH WILL!!']\n"
     ]
    }
   ],
   "source": [
    "# WE HAVE PRINT THE DATA THAT ARE IN TWO LISTS\n",
    "\n",
    "print(labelList[0:5],\"\\n\")\n",
    "\n",
    "print(textList[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.DataFrame({\\n    'label': labelList,\\n    'body_list': textList\\n})\\n\\ndf.head()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE BELOW CODE GAVE THE ERROR AND THE LENGTH OF BOTH THE LIST WAS NOT SAME\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "'''df = pd.DataFrame({\n",
    "    'label': labelList,\n",
    "    'body_list': textList\n",
    "})\n",
    "\n",
    "df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5571\n",
      "5570\n"
     ]
    }
   ],
   "source": [
    "# WE USE THIS CODE TO DETERMING THE LENGTH OF EACH LIST\n",
    "\n",
    "print(len(labelList))\n",
    "print(len(textList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'ham', 'ham', '']\n"
     ]
    }
   ],
   "source": [
    "# WE REALIZE THAT IN TEH END OF 'labelList' THERE IS EXTRA FIELD AT THE END\n",
    "\n",
    "print(labelList[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_list\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE CREATE A NEW DATA FRAME AND PASS BOTH THE LISTS IN IT IN THE FORM OF 'DICTIONARY'\n",
    "\n",
    "# WE USE THE BELOW CODE TO REMOVE THE LAST ONE LETTER FROM LABEL\n",
    "# ...WE PRINT ALL THE VALUES EXCEPT THE LAST VALUE\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'label': labelList[:-1],\n",
    "    'body_list': textList\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                          body_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE CAN USE THIS CODE TO DO THE SAME WHAT WE DID IN THE ABOVE CODE\n",
    "# .. WE CAN USE THE 'read_csv' METHOD ADN CAN TELL OUR CODE THAT THE VALUES ARE 'TAB' SEPARATED AND IT WILL PRINT\n",
    "# ..IN THE SAME WAY AS ABOVE\n",
    "\n",
    "\n",
    "# WE USE 'header = None'  BECAUSE THE RAW DATASET DOES NOT HAVE COLUMN NAMES, \n",
    "# ...SO IT WILL TAKE THE FIRST COLUMN AND ASSUME IT AS A COLUMN NAME\n",
    "\n",
    "dataset = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t', header = None)\n",
    "\n",
    "dataset.columns = ['Label', 'body_text']\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input data has 5568 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "# KNOW THE SHAPE OF DATA\n",
    "\n",
    "print(\" Input data has {} rows and {} columns\". format(len(dataset), len(dataset.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5568 rows, 746 are Spam and 4822 are ham\n"
     ]
    }
   ],
   "source": [
    "# KNOW 'SPAM' AND 'HAM'\n",
    "\n",
    "print(\"Out of {} rows, {} are Spam and {} are ham\".format(len(dataset),\n",
    "                                    len(dataset[dataset['Label'] == 'spam']),\n",
    "                                    len(dataset[dataset['Label'] == 'ham'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null in label: 0\n",
      "Number of null in label: 0\n"
     ]
    }
   ],
   "source": [
    "# KNOW THE MISSING DATA\n",
    "\n",
    "print(\"Number of null in label: {}\". format(dataset['Label'].isnull().sum()))\n",
    "\n",
    "print(\"Number of null in label: {}\". format(dataset['body_text'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions(RegEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0-9]+ THE '+' SIGN WILL HELP US TO RETURN THE NUMBER IN COMBINATION\n",
    "# [L-P] THIS WILL RETURN ONLY ONE LETTER\n",
    "# [L-P0-9]+   THIS WILL RETURN THE COMBINATION OF NUMBERS AND LETTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # This package allows us to use functions that we can use with RegEx\n",
    "\n",
    "test1 = 'This the not the messy sentence'\n",
    "test2 = ' This a messy    sentence    with lots of   spacs in it'\n",
    "test3 = ' This is a --- messy / sentence with-several. random charecters--- in it.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splittign the above sentences in the list of words. It is also called 'Tokenization'\n",
    "\n",
    "#### 'split' method looks for the spaces and 'findall' method looks for words.\n",
    "#### where  \\w  is based on words and  \\s  is beased on white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'the', 'not', 'the', 'messy', 'sentence']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This',\n",
       " 'a',\n",
       " 'messy',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'sentence',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'with',\n",
       " 'lots',\n",
       " 'of',\n",
       " '',\n",
       " '',\n",
       " 'spacs',\n",
       " 'in',\n",
       " 'it']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This',\n",
       " 'a',\n",
       " 'messy',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'spacs',\n",
       " 'in',\n",
       " 'it']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The '+' sign will tell Python to loook for one or more white spaces.\n",
    "\n",
    "re.split('\\s+', test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " '---',\n",
       " 'messy',\n",
       " '/',\n",
       " 'sentence',\n",
       " 'with-several.',\n",
       " 'random',\n",
       " 'charecters---',\n",
       " 'in',\n",
       " 'it.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "re.split('\\s+', test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'messy',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'several',\n",
       " 'random',\n",
       " 'charecters',\n",
       " 'in',\n",
       " 'it',\n",
       " '']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The '+' did not able to hande the special characters in the above statement\n",
    "# So we are using the different method '\\W+' this will search for non-word character\n",
    "\n",
    "\n",
    "re.split('\\W+', test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " '---',\n",
       " 'messy',\n",
       " '/',\n",
       " 'sentence',\n",
       " 'with-several.',\n",
       " 'random',\n",
       " 'charecters---',\n",
       " 'in',\n",
       " 'it.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Now we are reversing the above procedure - Instead of finding the\n",
    "spaces we are now trying to find the words.\n",
    "To do this we are using the 'findall' method.\n",
    "We have shange the small 's' to capital 'S' which will find the word instead\n",
    "of spaces between the words'''\n",
    "\n",
    "re.findall('\\S+', test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'messy',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'several',\n",
       " 'random',\n",
       " 'charecters',\n",
       " 'in',\n",
       " 'it']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we are replacing he capital 'W' with small 'w'\n",
    "\n",
    "re.findall('\\w+', test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPLACING THE SPECIFIC STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 'I try to follow PEP9 guidelines'\n",
    "sample2 = 'I try to follow PEP2 guidelines'\n",
    "sample3 = 'I try to follow PEP8 guidelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['try', 'to', 'follow', 'guidelines']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WHAT WE LEARN FROM THE BELOW CODE IS REGEX IS CASE SENSITIVE, IT IS LOOKING FOR VALUES ONLY WITH SMALL LETTERS\n",
    "# IF WE DEFINE THE VALUES IN SMALL LETTERS THEN IT WILL FIND ONLY SMALL LETTERS\n",
    "\n",
    "re.findall('[a-z]+', sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'PEP']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE BELOW RegEx PRINT THE ONLY CAPTIAL VALUES, BUT DIDN'T INCLUDE 8\n",
    "\n",
    "re.findall('[A-Z]+', sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PEP9']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE BELOW CODE IS CAPTURING THE NUMBER AS WELL IN PEP\n",
    "\n",
    "re.findall('[A-Z]+[0-9]', sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I try to follow PEP8 Python Styleguide guidelines'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THE BELOW CODE WILL FIND THE 'PEP8' AND REPLACE IT WITH 'PEP8 Python Styleguide'\n",
    "\n",
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', sample1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`There are many other RegEx methods, Like \n",
    "\n",
    "- re.search()\n",
    "- re.match()\n",
    "- re.fullmatch()\n",
    "- re.finditer()\n",
    "- re.escape()\n",
    "\n",
    "'The best way to learn RegEx is to open cheatsheet and to practice various RegEx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'The main steps involves in working on NLP are:\n",
    "\n",
    "'1. We have a Raw text which in completely unstructured, which we have to convert into the structured text.\n",
    "\n",
    "'2. Then we have to Tokenize the sentence, Tokexizing is the process of splitting the sentence into words. It makes easy for python, where to look  at.\n",
    "\n",
    "'3. Then we have to remove the Stop Words like I, an, the, and, of etc doesnt contribute much to the sentehce, so its better of we remove them. Removing these words helps python to concentrate on more important words and also reduces the data size.\n",
    "\n",
    "'4. Next step is called Vectorizing, it is the process in which we convert our text into Matrix on numbers, so that we can apply Machine Learnng algorithms to it, as machine learning models are good when it comes to work on numbers. In which Rows are the Messages and Columns are the Words.\n",
    "\n",
    "'5.Now that we have the Numeric Matrix, we can now apply machine learning algorithms to it, to  create a model.\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "1. Raw Text\n",
    "\n",
    "2. Tokenize\n",
    "\n",
    "3. Cleaning the Text\n",
    "\n",
    "4. Vectorize\n",
    "\n",
    "5. Apply Machine Learning Algorithms\n",
    "\n",
    "6. Spam Filter\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-precessing the text data\n",
    "`The first 3 steps are used in every pipeline, the 4th one is bit advanced and is not used in every project:\n",
    "\n",
    "`1. First we have to Remove Punctuations\n",
    "\n",
    "`2. Tokenize the words\n",
    "\n",
    "`3. Remove Stopwords\n",
    "\n",
    "`4. Lemmatize and Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# WE WANT TO SEE 100 CHARACTERS IN OUR PANDAS DATAFRAME\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "df = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t', header = None)\n",
    "\n",
    "df.columns = ['label', 'body_text']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ham  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "  I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...                                                                                                  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though                                                                                                  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.                                                                                                  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!                                                                                                  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...                                                                                                  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLEANING THE DATA AND PUTTING THE TOKENIZED VALUES IN ANOTHER COLUMN\n",
    "\n",
    "data_cleaned = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t')\n",
    "\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The python consider the below strings different\n",
    "\n",
    "\"I like NLP.\" == \"I like NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>[I, v, e,  , b, e, e, n,  , s, e, a, r, c, h, i, n, g,  , f, o, r,  , t, h, e,  , r, i, g, h, t,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[F, r, e, e,  , e, n, t, r, y,  , i, n,  , 2,  , a,  , w, k, l, y,  , c, o, m, p,  , t, o,  , w,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[N, a, h,  , I,  , d, o, n, t,  , t, h, i, n, k,  , h, e,  , g, o, e, s,  , t, o,  , u, s, f,  ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[E, v, e, n,  , m, y,  , b, r, o, t, h, e, r,  , i, s,  , n, o, t,  , l, i, k, e,  , t, o,  , s,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[I,  , H, A, V, E,  , A,  , D, A, T, E,  , O, N,  , S, U, N, D, A, Y,  , W, I, T, H,  , W, I, L, L]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \n",
       "0  [I, v, e,  , b, e, e, n,  , s, e, a, r, c, h, i, n, g,  , f, o, r,  , t, h, e,  , r, i, g, h, t,...  \n",
       "1  [F, r, e, e,  , e, n, t, r, y,  , i, n,  , 2,  , a,  , w, k, l, y,  , c, o, m, p,  , t, o,  , w,...  \n",
       "2  [N, a, h,  , I,  , d, o, n, t,  , t, h, i, n, k,  , h, e,  , g, o, e, s,  , t, o,  , u, s, f,  ,...  \n",
       "3  [E, v, e, n,  , m, y,  , b, r, o, t, h, e, r,  , i, s,  , n, o, t,  , l, i, k, e,  , t, o,  , s,...  \n",
       "4  [I,  , H, A, V, E,  , A,  , D, A, T, E,  , O, N,  , S, U, N, D, A, Y,  , W, I, T, H,  , W, I, L, L]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So if we see anything like above, we want to remove it.\n",
    "'''So to clean this up, we will take the list of punctuations. And set a \n",
    "Loop on it. So whenever the python see the punctuations like this, it will \n",
    "remove it. We are using \"List Comprihension to do this\". The List Comprihension\n",
    "makes it easy to cycle through elements, check it if they meet some condition\n",
    "and return the output '''\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunctuation = [char for char in text if char not in string.punctuation]\n",
    "    \n",
    "    return text_nopunctuation\n",
    "    \n",
    "df['body_text_clean'] = df['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df.head()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "2                                          Nah I dont think he goes to usf he lives around here though  \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent  \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE CAN JOIN THE SENTENCES BY USING THE JOIN FUNCTION BELOW\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunctuation = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    return text_nopunctuation\n",
    "    \n",
    "df['body_text_clean'] = df['body_text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tekenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "             \n",
    "df['body_text_tokenized'] = df['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "             \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "  body_text_nostop  \n",
       "0             None  \n",
       "1             None  \n",
       "2             None  \n",
       "3             None  \n",
       "4             None  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = (word for word in tokenized_list if word not in stopword)\n",
    "    \n",
    "df['body_text_nostop'] = df['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter: 2 - Supplemental Data Cleaning\n",
    "\n",
    "In this chapter we will study about the Stemming and Lemmatizing\n",
    "\n",
    "Stemming: It is the process of reducing inflected(or something derived) words to their word stem or root. Or in othr words it is th process of chopping off the end of the word and leave only the base. \n",
    "\n",
    "It helps in reducing the words that python has to look at or consider.\n",
    "\n",
    "For eg: \n",
    "\n",
    "Stemming/ Stemmed would be changed to Stem\n",
    "\n",
    "Electricity/ Electrical ----> Electr\n",
    "\n",
    "Earries/ Berry ----> Berri\n",
    "\n",
    "Connection/Connected/Connective ----> Connect\n",
    "\n",
    "\n",
    "Lemmatizing: It is the process of grouping together the inflected forms of a word so they can be analyzed as a single term, identified by the word's lemma. The 'lemma' is a canonical from of set of words.\n",
    "\n",
    "How is Lemmatizing and Stemming are differnet:\n",
    "\n",
    "- The goal of both is to condense derived words into their base forms.\n",
    "\n",
    "- stemming is fast as it simply chops off the end of a word using heuristics, without any understanding of the context in which a word is used. Becuase of this the stemming may or may not return the actual word in the dictionary. It is usually less accurate, but it is faster.\n",
    "\n",
    "- Lemmatizing is more accurate as it uses more informed analysis to create groups of words with similar meaning based on the context around the word. It always returns the dictionary words, so it is more accurate, but its more computationally extensive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "grow\n",
      "grow\n"
     ]
    }
   ],
   "source": [
    "# USING THE STEM FUNCTION\n",
    "\n",
    "print(ps.stem('grows'))\n",
    "print(ps.stem('growing'))\n",
    "print(ps.stem('grow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('run'))\n",
    "print(ps.stem('running'))\n",
    "print(ps.stem('runner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "dt = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "dt.columns = ['lable', 'body_text']\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                     body_text_nonstop  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "dt['body_text_nonstop'] = dt['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nonstop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                     body_text_nonstop  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                     body_text_stemmed  \n",
       "0  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "1                                                   [nah, dont, think, goe, usf, live, around, though]  \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, mell, mell, oru, minnaminungint, nurungu, vettam, set, callertun, caller, press, ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(tokexized_text):\n",
    "    text = [ps.stem(word) for word in tokexized_text]\n",
    "    return text\n",
    "\n",
    "dt['body_text_stemmed'] = dt['body_text_nonstop'].apply(lambda x:stemming(x))\n",
    "\n",
    "dt.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Text\n",
    "\n",
    "We gonna do it in 2 steps:\n",
    "\n",
    "First we gonna use Lemmatizer on specific words to understand how it works\n",
    "\n",
    "Second, we gonna apply it on SMS Spam data set to clean it up further.\n",
    "\n",
    "There are many Lemmatizer, but we gonna use 'WordNet' Lemmatizer. It is the collection of nouns, verbs, adjectives and adverbs, that are grouped together in the set of synonyms and each expressing destince concept.\n",
    "\n",
    "It runs on the corpus of synonyms. So if we give it a word, it will track it with its synonyms and will represent the distince concept of those group of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import wordnet\n",
    "\n",
    "#Using WordNet Lemmatizer and PorterStemmer\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "mean\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('meanness'))\n",
    "print(ps.stem('meaning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanness\n",
      "meaning\n"
     ]
    }
   ],
   "source": [
    "# THE LEMMATIZER SERCHES THE WHOLE DICTIONARY TO FIND THE RELATED WORDS AND THEN GIVES US AN ACTUAL WORD\n",
    "# AND IF TH WORD IS NOT IN THE DICTIONARY, THEN IT WILL JUST RETURN THE ORIGINAL WORD.\n",
    "\n",
    "print(wn.lemmatize('meanness'))\n",
    "print(wn.lemmatize('meaning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goos\n",
      "gees\n"
     ]
    }
   ],
   "source": [
    "# APPLYING STEMMING ON ANOTHER EXAMPLE\n",
    "\n",
    "print(ps.stem('goose'))\n",
    "print(ps.stem('geese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goose\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "print(wn.lemmatize('goose'))\n",
    "print(wn.lemmatize('geese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "dt = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "dt.columns = ['lable', 'body_text']\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'Natural', 'LanguageProccessing']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "str=\"I love Natural Language.Proccessing\"\n",
    "clean_text(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                     body_text_nonstop  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "dt['body_text_nonstop'] = dt['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nonstop</th>\n",
       "      <th>lemmatized_body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c...</td>\n",
       "      <td>[winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170...</td>\n",
       "      <td>[winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...</td>\n",
       "      <td>[mobile, 11, months, u, r, entitled, update, latest, colour, mobiles, camera, free, call, mobile...</td>\n",
       "      <td>[mobile, 11, month, u, r, entitled, update, latest, colour, mobile, camera, free, call, mobile, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...</td>\n",
       "      <td>[im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]</td>\n",
       "      <td>[im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, ...</td>\n",
       "      <td>[six, chances, win, cash, 100, 20000, pounds, txt, csh11, send, 87575, cost, 150pday, 6days, 16,...</td>\n",
       "      <td>[six, chance, win, cash, 100, 20000, pound, txt, csh11, send, 87575, cost, 150pday, 6days, 16, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM...</td>\n",
       "      <td>[urgent, 1, week, free, membership, 100000, prize, jackpot, txt, word, claim, 81010, tc, wwwdbuk...</td>\n",
       "      <td>[urgent, 1, week, free, membership, 100000, prize, jackpot, txt, word, claim, 81010, tc, wwwdbuk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "5  spam   \n",
       "6  spam   \n",
       "7   ham   \n",
       "8  spam   \n",
       "9  spam   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "5  WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To c...   \n",
       "6  Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...   \n",
       "7  I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...   \n",
       "8  SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, ...   \n",
       "9  URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM...   \n",
       "\n",
       "                                                                                     body_text_nonstop  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "5  [winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170...   \n",
       "6  [mobile, 11, months, u, r, entitled, update, latest, colour, mobiles, camera, free, call, mobile...   \n",
       "7     [im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]   \n",
       "8  [six, chances, win, cash, 100, 20000, pounds, txt, csh11, send, 87575, cost, 150pday, 6days, 16,...   \n",
       "9  [urgent, 1, week, free, membership, 100000, prize, jackpot, txt, word, claim, 81010, tc, wwwdbuk...   \n",
       "\n",
       "                                                                                  lemmatized_body_text  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                    [nah, dont, think, go, usf, life, around, though]  \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...  \n",
       "5  [winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170...  \n",
       "6  [mobile, 11, month, u, r, entitled, update, latest, colour, mobile, camera, free, call, mobile, ...  \n",
       "7     [im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]  \n",
       "8  [six, chance, win, cash, 100, 20000, pound, txt, csh11, send, 87575, cost, 150pday, 6days, 16, t...  \n",
       "9  [urgent, 1, week, free, membership, 100000, prize, jackpot, txt, word, claim, 81010, tc, wwwdbuk...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "dt['lemmatized_body_text'] = dt['body_text_nonstop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "dt.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Vectorizing Raw Data\n",
    "\n",
    "Vectorizing is the process of converting the text into numeric form, so that it can we easily understood by our machine learning model. In other words, it is the process of encoding text as integers to create feature vectors.\n",
    "\n",
    "Feature Vector: It is an n-dimensional vector of numerical features that represent some object. In our case we would be taking a text message and converting it into numeric vector, that represents that text message.\n",
    "\n",
    "Document Term Matrix:\n",
    "\n",
    "In spam detection, our model actually counts the occurance of certin word and determines whether the message is spam or ham. \n",
    "\n",
    "#### The Vectorization has mainly 3 types:\n",
    "\n",
    "1. Count Vectorization: In which we count the occurance of certain word.\n",
    "\n",
    "2. N-grams: \n",
    "\n",
    "3. Term Frequency - Inverse Document Frequency(TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count Vectorization implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "dt = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "dt.columns = ['lable', 'body_text']\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to remove punctuation, tokenize, remove stowords and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE BELOW CODE WE ARE USING THE STEMMER TO STEM THE WORDS AND WE ARE NOT USING 'LAMBDA' FUNCTION HERE\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text) \n",
    "    \n",
    "# USING THE STEMMBR IN THE BELOW LINE  \n",
    "\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "#dt['body_text_nonstop'] = dt['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "#dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are total 5567 text messages and 8334 unique words in those text messages \n",
      " Which means our document term matrix has 5567 rows and 8334 columns \n",
      "\n",
      "(5567, 8334) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = clean_text)\n",
    "\n",
    "# WE ARE USING FIT_TRANSFORM METHOD TO FIT OUR MODEL AND TRANSFORM IT, \n",
    "# IF WE USE ONLY 'FIT' THEN IT WILL NOT TRANSFORM THE DATA\n",
    "\n",
    "X_counts = count_vect.fit_transform(dt['body_text'])\n",
    "\n",
    "print(\" There are total 5567 text messages and 8334 unique words in those text messages \\n Which means our document term matrix has 5567 rows and 8334 columns\", '\\n')\n",
    "\n",
    "print(X_counts.shape, '\\n')\n",
    "\n",
    "#print(count_vect.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying CountVectorizer on small sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 20 rows and 212 columns:  (20, 212) \n",
      "\n",
      "['08002986030', '08452810075over18', '09061701461', '1', '100', '100000', '11', '12', '150pday', '16', '2', '20000', '2005', '21st', '3', '4', '4403ldnw1a7rw18', '4txtú120', '6day', '81010', '87077', '87121', '87575', '9', '900', 'A', 'As', 'Co', 'Eh', 'FA', 'HL', 'He', 'I', 'Im', 'Is', 'No', 'ON', 'Oh', 'R', 'So', 'TC', 'To', 'U', 'aft', 'aid', 'alreadi', 'alright', 'anymor', 'appli', 'ard', 'around', 'b', 'brother', 'call', 'caller', 'callertun', 'camera', 'cash', 'chanc', 'claim', 'click', 'code', 'colour', 'comin', 'comp', 'copi', 'cost', 'credit', 'cri', 'csh11', 'cup', 'custom', 'da', 'date', 'dont', 'eg', 'england', 'enough', 'entitl', 'entri', 'even', 'feel', 'ffffffffff', 'final', 'fine', 'finish', 'first', 'free', 'friend', 'from', 'go', 'goalsteam', 'goe', 'gonna', 'gota', 'ha', 'had', 'have', 'home', 'hour', 'httpwap', 'info', 'ive', 'jackpot', 'joke', 'k', 'kim', 'kl341', 'lar', 'latest', 'lccltd', 'like', 'link', 'live', 'lor', 'lunch', 'macedonia', 'make', 'may', 'meet', 'mell', 'membership', 'messag', 'minnaminungint', 'miss', 'mobil', 'month', 'nah', 'name', 'nation', 'naughti', 'network', 'news', 'next', 'nurungu', 'oru', 'patent', 'pay', 'per', 'pobox', 'poboxox36504w45wq', 'pound', 'press', 'prize', 'questionstd', 'ratetc', 'receiv', 'receivea', 'rememb', 'repli', 'request', 'reward', 'scotland', 'select', 'send', 'serious', 'set', 'six', 'smth', 'soon', 'sooner', 'speak', 'spell', 'stock', 'str', 'stuff', 'sunday', 'talk', 'team', 'text', 'that', 'the', 'then', 'they', 'think', 'though', 'tkt', 'today', 'tonight', 'treat', 'tri', 'trywal', 'tsandc', 'txt', 'u', 'updat', 'ur', 'urgent', 'use', 'usf', 'v', 'valid', 'valu', 'vettam', 'want', 'wap', 'watch', 'way', 'week', 'wet', 'will', 'win', 'winner', 'with', 'wkli', 'word', 'wwwdbuknet', 'xxxmobilemovieclub', 'xxxmobilemovieclubcomnqjkgighjjgcbl', 'ye', 'you', 'ü']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_sample = dt[0:20]\n",
    "\n",
    "count_vect_sample = CountVectorizer(analyzer = clean_text)\n",
    "\n",
    "# WE ARE USING FIT_TRANSFORM METHOD TO FIT OUR MODEL AND TRANSFORM IT, \n",
    "# IF WE USE ONLY 'FIT' THEN IT WILL NOT TRANSFORM THE DATA\n",
    "\n",
    "X_counts_sample = count_vect_sample.fit_transform(data_sample['body_text'])\n",
    "\n",
    "\n",
    "print(\"There are total 20 rows and 212 columns: \", X_counts_sample.shape, '\\n')\n",
    "\n",
    "print(count_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data output what we can see above from the CountVectorizer is called Sparse Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x212 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 242 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 20 rows and 212 columns in our Document Term Matrix:  (20, 212) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    ...  202  203  204  205  \\\n",
       "0     0    1    0    0    0    0    0    0    0    0  ...    0    0    1    0   \n",
       "1     0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2     0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3     0    0    0    0    0    0    0    0    0    0  ...    0    1    0    0   \n",
       "4     0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "5     0    0    1    0    0    0    0    1    0    0  ...    1    0    0    0   \n",
       "6     1    0    0    0    0    0    1    0    0    0  ...    0    0    0    0   \n",
       "7     0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "8     0    0    0    0    1    0    0    0    1    1  ...    0    0    0    0   \n",
       "9     0    0    0    1    0    1    0    0    0    0  ...    0    0    0    1   \n",
       "10    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "11    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "12    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "13    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "14    0    0    0    0    0    0    0    0    0    1  ...    0    0    0    0   \n",
       "15    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "16    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "17    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "18    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "19    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "    206  207  208  209  210  211  \n",
       "0     0    0    0    0    0    0  \n",
       "1     0    0    0    0    0    0  \n",
       "2     0    0    0    0    0    0  \n",
       "3     0    0    0    0    0    0  \n",
       "4     0    0    0    0    0    0  \n",
       "5     0    0    0    0    0    0  \n",
       "6     0    0    0    0    0    0  \n",
       "7     0    0    0    0    0    0  \n",
       "8     0    0    0    0    0    0  \n",
       "9     1    0    0    0    1    0  \n",
       "10    0    1    1    0    0    0  \n",
       "11    0    0    0    0    0    0  \n",
       "12    0    0    0    1    0    0  \n",
       "13    0    0    0    0    0    0  \n",
       "14    0    0    0    0    0    0  \n",
       "15    0    0    0    0    0    0  \n",
       "16    0    0    0    0    0    0  \n",
       "17    0    0    0    0    0    1  \n",
       "18    0    0    0    0    0    0  \n",
       "19    0    0    0    0    0    0  \n",
       "\n",
       "[20 rows x 212 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO PRINT OUT THE MATRIX, WE HAVE TO EXPAND THIS SPARSE MATRIX TO A COLLECTION\n",
    "# ...OF ARRAY AND THEN CREATE A DATAFROM FROM THEM.\n",
    "\n",
    "x_counts_df = pd.DataFrame(X_counts_sample.toarray())\n",
    "\n",
    "print(\"There are total 20 rows and 212 columns in our Document Term Matrix: \", X_counts_sample.shape, '\\n')\n",
    "\n",
    "x_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to see what word in each column actually represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08002986030</th>\n",
       "      <th>08452810075over18</th>\n",
       "      <th>09061701461</th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>100000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>150pday</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>winner</th>\n",
       "      <th>with</th>\n",
       "      <th>wkli</th>\n",
       "      <th>word</th>\n",
       "      <th>wwwdbuknet</th>\n",
       "      <th>xxxmobilemovieclub</th>\n",
       "      <th>xxxmobilemovieclubcomnqjkgighjjgcbl</th>\n",
       "      <th>ye</th>\n",
       "      <th>you</th>\n",
       "      <th>ü</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    08002986030  08452810075over18  09061701461  1  100  100000  11  12  \\\n",
       "0             0                  1            0  0    0       0   0   0   \n",
       "1             0                  0            0  0    0       0   0   0   \n",
       "2             0                  0            0  0    0       0   0   0   \n",
       "3             0                  0            0  0    0       0   0   0   \n",
       "4             0                  0            0  0    0       0   0   0   \n",
       "5             0                  0            1  0    0       0   0   1   \n",
       "6             1                  0            0  0    0       0   1   0   \n",
       "7             0                  0            0  0    0       0   0   0   \n",
       "8             0                  0            0  0    1       0   0   0   \n",
       "9             0                  0            0  1    0       1   0   0   \n",
       "10            0                  0            0  0    0       0   0   0   \n",
       "11            0                  0            0  0    0       0   0   0   \n",
       "12            0                  0            0  0    0       0   0   0   \n",
       "13            0                  0            0  0    0       0   0   0   \n",
       "14            0                  0            0  0    0       0   0   0   \n",
       "15            0                  0            0  0    0       0   0   0   \n",
       "16            0                  0            0  0    0       0   0   0   \n",
       "17            0                  0            0  0    0       0   0   0   \n",
       "18            0                  0            0  0    0       0   0   0   \n",
       "19            0                  0            0  0    0       0   0   0   \n",
       "\n",
       "    150pday  16  ...  winner  with  wkli  word  wwwdbuknet  \\\n",
       "0         0   0  ...       0     0     1     0           0   \n",
       "1         0   0  ...       0     0     0     0           0   \n",
       "2         0   0  ...       0     0     0     0           0   \n",
       "3         0   0  ...       0     1     0     0           0   \n",
       "4         0   0  ...       0     0     0     0           0   \n",
       "5         0   0  ...       1     0     0     0           0   \n",
       "6         0   0  ...       0     0     0     0           0   \n",
       "7         0   0  ...       0     0     0     0           0   \n",
       "8         1   1  ...       0     0     0     0           0   \n",
       "9         0   0  ...       0     0     0     1           1   \n",
       "10        0   0  ...       0     0     0     0           0   \n",
       "11        0   0  ...       0     0     0     0           0   \n",
       "12        0   0  ...       0     0     0     0           0   \n",
       "13        0   0  ...       0     0     0     0           0   \n",
       "14        0   1  ...       0     0     0     0           0   \n",
       "15        0   0  ...       0     0     0     0           0   \n",
       "16        0   0  ...       0     0     0     0           0   \n",
       "17        0   0  ...       0     0     0     0           0   \n",
       "18        0   0  ...       0     0     0     0           0   \n",
       "19        0   0  ...       0     0     0     0           0   \n",
       "\n",
       "    xxxmobilemovieclub  xxxmobilemovieclubcomnqjkgighjjgcbl  ye  you  ü  \n",
       "0                    0                                    0   0    0  0  \n",
       "1                    0                                    0   0    0  0  \n",
       "2                    0                                    0   0    0  0  \n",
       "3                    0                                    0   0    0  0  \n",
       "4                    0                                    0   0    0  0  \n",
       "5                    0                                    0   0    0  0  \n",
       "6                    0                                    0   0    0  0  \n",
       "7                    0                                    0   0    0  0  \n",
       "8                    0                                    0   0    0  0  \n",
       "9                    0                                    0   0    1  0  \n",
       "10                   1                                    1   0    0  0  \n",
       "11                   0                                    0   0    0  0  \n",
       "12                   0                                    0   1    0  0  \n",
       "13                   0                                    0   0    0  0  \n",
       "14                   0                                    0   0    0  0  \n",
       "15                   0                                    0   0    0  0  \n",
       "16                   0                                    0   0    0  0  \n",
       "17                   0                                    0   0    0  1  \n",
       "18                   0                                    0   0    0  0  \n",
       "19                   0                                    0   0    0  0  \n",
       "\n",
       "[20 rows x 212 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_counts_df.columns = count_vect_sample.get_feature_names()\n",
    "\n",
    "x_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. N-Grams implementation\n",
    "\n",
    "The N-Grams process createa a Document-term matrix where counts still occupy the cell but instead of the columns representing single teerms, they represent all combinations of adjacent words of length 'n' in your text.\n",
    "\n",
    "Tha N-Grams are in the form of Bi-Grams, Tri-Grams, Four-Grams upto N-Grams.\n",
    "\n",
    "For Eg, \"I like NLP\" in Bi-Gram would be \"I like\", \"like NLP\"\n",
    "\n",
    "\n",
    "\"NLP is an interesting topic\"\n",
    "\n",
    "| n | Name      | Tokens                                                         |\n",
    "|---|-----------|----------------------------------------------------------------|\n",
    "| 2 | bigram    | [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"]      |\n",
    "| 3 | trigram   | [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] |\n",
    "| 4 | four-gram | [\"nlp is an interesting\", \"is an interesting topic\"]    |\n",
    "\n",
    "#### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "dt = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "dt.columns = ['lable', 'body_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to remove punctuation, tokenize, remove stowords and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entri 2 wkli comp win FA cup final tkt 21st may 2005 text FA 87121 receiv entri questionstd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah I dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even brother like speak they treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I have A date ON sunday with will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>As per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi fri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                          cleaned_text  \n",
       "0  free entri 2 wkli comp win FA cup final tkt 21st may 2005 text FA 87121 receiv entri questionstd...  \n",
       "1                                                          nah I dont think goe usf live around though  \n",
       "2                                                   even brother like speak they treat like aid patent  \n",
       "3                                                                    I have A date ON sunday with will  \n",
       "4  As per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi fri...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "# USING THE STEMMBR IN THE BELOW LINE   \n",
    "    \n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "dt['cleaned_text'] = dt['body_text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying CountVectorizer with N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 34114) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# IN THE BELOW CODE WE CAN CHANGE TO RANGE OF NGRAM TO (1,3), (2,3), (1,2) ETC. TO MAKE IT TRI-GRAM, FOUR-GRAM ETC\n",
    "\n",
    "ngram_vect = CountVectorizer(ngram_range= (2,2))\n",
    "\n",
    "x_counts = ngram_vect.fit_transform(dt['cleaned_text'])\n",
    "\n",
    "print(x_counts.shape, '\\n')\n",
    "\n",
    "#print(ngram_vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying CountVectorizer with N-Grams to a smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 217) \n",
      "\n",
      "['09061701461 claim', '100 20000', '100000 prize', '11 month', '12 hour', '150pday 6day', '16 tsandc', '20000 pound', '2005 text', '21st may', '4txtú120 poboxox36504w45wq', '6day 16', '81010 tc', '87077 eg', '87077 trywal', '87121 receiv', '87575 cost', '900 prize', 'aft finish', 'aid patent', 'alright way', 'anymor tonight', 'appli 08452810075over18', 'appli repli', 'ard smth', 'around though', 'as per', 'as valu', 'brother like', 'call 09061701461', 'call the', 'caller press', 'callertun caller', 'camera free', 'cash from', 'chanc win', 'claim call', 'claim code', 'claim no', 'click httpwap', 'click wap', 'co free', 'code kl341', 'colour mobil', 'comp win', 'copi friend', 'cost 150pday', 'credit click', 'cri enough', 'csh11 send', 'cup final', 'custom select', 'da stock', 'date on', 'dont miss', 'dont think', 'dont want', 'eg england', 'eh rememb', 'england 87077', 'england macedonia', 'enough today', 'entitl updat', 'entri questionstd', 'entri wkli', 'even brother', 'fa 87121', 'fa cup', 'feel that', 'ffffffffff alright', 'final tkt', 'fine way', 'finish lunch', 'finish ur', 'first lar', 'free 08002986030', 'free call', 'free entri', 'free membership', 'friend callertun', 'from 100', 'go str', 'go tri', 'goalsteam news', 'goe usf', 'gonna home', 'ha ha', 'ha joke', 'had mobil', 'have date', 'he naughti', 'hl info', 'home soon', 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl', 'im gonna', 'is serious', 'ive cri', 'jackpot txt', 'kim watch', 'kl341 valid', 'lar then', 'latest colour', 'lccltd pobox', 'like aid', 'like speak', 'link next', 'live around', 'lor ard', 'lor finish', 'lunch alreadi', 'lunch go', 'macedonia dont', 'make wet', 'may 2005', 'meet sooner', 'mell mell', 'mell oru', 'membership 100000', 'messag click', 'minnaminungint nurungu', 'miss goalsteam', 'mobil 11', 'mobil camera', 'mobil updat', 'month entitl', 'month ha', 'nah dont', 'name ye', 'nation team', 'naughti make', 'network custom', 'news txt', 'next txt', 'no 81010', 'nurungu vettam', 'oh kim', 'on sunday', 'oru minnaminungint', 'pay first', 'per request', 'pobox 4403ldnw1a7rw18', 'poboxox36504w45wq 16', 'pound txt', 'press copi', 'prize jackpot', 'prize reward', 'questionstd txt', 'ratetc appli', 'receiv entri', 'receivea 900', 'rememb spell', 'repli hl', 'request mell', 'reward to', 'scotland 4txtú120', 'select receivea', 'send 87575', 'serious spell', 'set callertun', 'six chanc', 'smth lor', 'so pay', 'soon dont', 'speak they', 'spell name', 'stock comin', 'str lor', 'stuff anymor', 'sunday with', 'talk stuff', 'tc wwwdbuknet', 'team 87077', 'text fa', 'that way', 'the mobil', 'then da', 'they treat', 'think goe', 'tkt 21st', 'to claim', 'to use', 'tonight ive', 'treat like', 'tri month', 'trywal scotland', 'tsandc appli', 'txt csh11', 'txt messag', 'txt ratetc', 'txt ur', 'txt word', 'updat co', 'updat latest', 'ur lunch', 'ur nation', 'urgent you', 'use credit', 'usf live', 'valid 12', 'valu network', 'vettam set', 'want talk', 'wap link', 'way feel', 'way gota', 'way meet', 'week free', 'win cash', 'win fa', 'winner as', 'with will', 'wkli comp', 'word claim', 'wwwdbuknet lccltd', 'xxxmobilemovieclub to', 'ye he', 'you week']\n"
     ]
    }
   ],
   "source": [
    "data_sample = dt[0:20]\n",
    "\n",
    "# IN THE BELOW CODE WE CAN CHANGE TO RANGE OF NGRAM TO (1,3), (2,3), (1,2) ETC. TO MAKE IT TRI-GRAM, FOUR-GRAM ETC\n",
    "\n",
    "ngram_vect_sample = CountVectorizer(ngram_range= (2,2))\n",
    "\n",
    "x_counts_sample = ngram_vect_sample.fit_transform(data_sample['cleaned_text'])\n",
    "\n",
    "print(x_counts_sample.shape, '\\n')\n",
    "\n",
    "print(ngram_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing output Sparse Matrix\n",
    "\n",
    "Sparse Matrix: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-sero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# TO PRINT OUT THE MATRIX, WE HAVE TO EXPAND THIS SPARSE MATRIX TO A COLLECTION\\n# ...OF ARRAY AND THEN CREATE A DATAFROM FROM THEM.\\n\\nx_counts_df = pd.DataFrame(X_counts_sample.toarray())\\n\\nx_counts_df.columns = ngram_vect_sample.get_feature_names()\\n\\nx_counts_df\\n\\n# len(x_counts_df, x_counts_df.columns)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# TO PRINT OUT THE MATRIX, WE HAVE TO EXPAND THIS SPARSE MATRIX TO A COLLECTION\n",
    "# ...OF ARRAY AND THEN CREATE A DATAFROM FROM THEM.\n",
    "\n",
    "x_counts_df = pd.DataFrame(X_counts_sample.toarray())\n",
    "\n",
    "x_counts_df.columns = ngram_vect_sample.get_feature_names()\n",
    "\n",
    "x_counts_df\n",
    "\n",
    "# len(x_counts_df, x_counts_df.columns)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency- Inverse Document Frequency)\n",
    "\n",
    "TF-IDF creates a Document Term Matrix, when there is one row per text message and the colums represents teh single unique terms. But instead of cells representing the count, the cells represents the weightings, that is meant to identify how important is the word to an individual text message.\n",
    "\n",
    "It is expressed by the formuls:\n",
    "\n",
    "Wi,f = TF(i,j) x log(N/df(i))\n",
    "\n",
    "where,\n",
    "\n",
    "TF(i,j) = Number of times 'i' occure in message 'j' divided by total number of words in 'j'\n",
    "\n",
    "\n",
    "df(I) = Number of documents containing i( This part measures how frequently word occurs across all other text messages)\n",
    "\n",
    "N = total number of documents\n",
    "\n",
    "Eg: \" I like NLP\"   and if we are looking at NLP, then 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "dt = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "dt.columns = ['lable', 'body_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to remove punctuation, tokenize, remove stowords and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "# USING THE STEMMBR IN THE BELOW LINE   \n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "#dt['cleaned_text'] = dt['body_text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "#dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying TfidfVactorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 8337)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf = tfidf_vect.fit_transform(df['body_text'])\n",
    "\n",
    "print(x_tfidf.shape)\n",
    "\n",
    "#print(tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying TfidfVectorizer on small sample for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 212)\n",
      "['08002986030', '08452810075over18', '09061701461', '1', '100', '100000', '11', '12', '150pday', '16', '2', '20000', '2005', '21st', '3', '4', '4403ldnw1a7rw18', '4txtú120', '6day', '81010', '87077', '87121', '87575', '9', '900', 'A', 'As', 'Co', 'Eh', 'FA', 'HL', 'He', 'I', 'Im', 'Is', 'No', 'ON', 'Oh', 'R', 'So', 'TC', 'To', 'U', 'aft', 'aid', 'alreadi', 'alright', 'anymor', 'appli', 'ard', 'around', 'b', 'brother', 'call', 'caller', 'callertun', 'camera', 'cash', 'chanc', 'claim', 'click', 'code', 'colour', 'comin', 'comp', 'copi', 'cost', 'credit', 'cri', 'csh11', 'cup', 'custom', 'da', 'date', 'dont', 'eg', 'england', 'enough', 'entitl', 'entri', 'even', 'feel', 'ffffffffff', 'final', 'fine', 'finish', 'first', 'free', 'friend', 'from', 'go', 'goalsteam', 'goe', 'gonna', 'gota', 'ha', 'had', 'have', 'home', 'hour', 'httpwap', 'info', 'ive', 'jackpot', 'joke', 'k', 'kim', 'kl341', 'lar', 'latest', 'lccltd', 'like', 'link', 'live', 'lor', 'lunch', 'macedonia', 'make', 'may', 'meet', 'mell', 'membership', 'messag', 'minnaminungint', 'miss', 'mobil', 'month', 'nah', 'name', 'nation', 'naughti', 'network', 'news', 'next', 'nurungu', 'oru', 'patent', 'pay', 'per', 'pobox', 'poboxox36504w45wq', 'pound', 'press', 'prize', 'questionstd', 'ratetc', 'receiv', 'receivea', 'rememb', 'repli', 'request', 'reward', 'scotland', 'select', 'send', 'serious', 'set', 'six', 'smth', 'soon', 'sooner', 'speak', 'spell', 'stock', 'str', 'stuff', 'sunday', 'talk', 'team', 'text', 'that', 'the', 'then', 'they', 'think', 'though', 'tkt', 'today', 'tonight', 'treat', 'tri', 'trywal', 'tsandc', 'txt', 'u', 'updat', 'ur', 'urgent', 'use', 'usf', 'v', 'valid', 'valu', 'vettam', 'want', 'wap', 'watch', 'way', 'week', 'wet', 'will', 'win', 'winner', 'with', 'wkli', 'word', 'wwwdbuknet', 'xxxmobilemovieclub', 'xxxmobilemovieclubcomnqjkgighjjgcbl', 'ye', 'you', 'ü']\n"
     ]
    }
   ],
   "source": [
    "data_sample = dt[0:20]\n",
    "\n",
    "tfidf_vect_sample = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['body_text'])\n",
    "\n",
    "print(x_tfidf_sample.shape)\n",
    "print(tfidf_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized output sparse matrices\n",
    "\n",
    "On running the code we will see the various numbers, which shows the frequence of occurance of various words.\n",
    "\n",
    "If we see the number '12' then the number 0.2226 tell us that the 12 is occuring more in 5th text message and for '11' we have the number 0.189543, which shows its occurance in the text message 11.\n",
    "\n",
    "The higher the number, that means the more a letter is occuring in the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08002986030</th>\n",
       "      <th>08452810075over18</th>\n",
       "      <th>09061701461</th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>100000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>150pday</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>winner</th>\n",
       "      <th>with</th>\n",
       "      <th>wkli</th>\n",
       "      <th>word</th>\n",
       "      <th>wwwdbuknet</th>\n",
       "      <th>xxxmobilemovieclub</th>\n",
       "      <th>xxxmobilemovieclubcomnqjkgighjjgcbl</th>\n",
       "      <th>ye</th>\n",
       "      <th>you</th>\n",
       "      <th>ü</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.36447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.189543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189543</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.219424</td>\n",
       "      <td>0.192877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.238188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238188</td>\n",
       "      <td>0.238188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238188</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.265144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    08002986030  08452810075over18  09061701461         1       100    100000  \\\n",
       "0      0.000000           0.198986       0.0000  0.000000  0.000000  0.000000   \n",
       "1      0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "2      0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "3      0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "4      0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "5      0.000000           0.000000       0.2226  0.000000  0.000000  0.000000   \n",
       "6      0.189543           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "7      0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "8      0.000000           0.000000       0.0000  0.000000  0.219424  0.000000   \n",
       "9      0.000000           0.000000       0.0000  0.238188  0.000000  0.238188   \n",
       "10     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "11     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "12     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "13     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "14     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "15     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "16     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "17     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "18     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "19     0.000000           0.000000       0.0000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          11      12   150pday        16  ...  winner     with      wkli  \\\n",
       "0   0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.198986   \n",
       "1   0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "2   0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "3   0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.36447  0.000000   \n",
       "4   0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "5   0.000000  0.2226  0.000000  0.000000  ...  0.2226  0.00000  0.000000   \n",
       "6   0.189543  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "7   0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "8   0.000000  0.0000  0.219424  0.192877  ...  0.0000  0.00000  0.000000   \n",
       "9   0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "10  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "11  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "12  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "13  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "14  0.000000  0.0000  0.000000  0.185730  ...  0.0000  0.00000  0.000000   \n",
       "15  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "16  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "17  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "18  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "19  0.000000  0.0000  0.000000  0.000000  ...  0.0000  0.00000  0.000000   \n",
       "\n",
       "        word  wwwdbuknet  xxxmobilemovieclub  \\\n",
       "0   0.000000    0.000000            0.000000   \n",
       "1   0.000000    0.000000            0.000000   \n",
       "2   0.000000    0.000000            0.000000   \n",
       "3   0.000000    0.000000            0.000000   \n",
       "4   0.000000    0.000000            0.000000   \n",
       "5   0.000000    0.000000            0.000000   \n",
       "6   0.000000    0.000000            0.000000   \n",
       "7   0.000000    0.000000            0.000000   \n",
       "8   0.000000    0.000000            0.000000   \n",
       "9   0.238188    0.238188            0.000000   \n",
       "10  0.000000    0.000000            0.265144   \n",
       "11  0.000000    0.000000            0.000000   \n",
       "12  0.000000    0.000000            0.000000   \n",
       "13  0.000000    0.000000            0.000000   \n",
       "14  0.000000    0.000000            0.000000   \n",
       "15  0.000000    0.000000            0.000000   \n",
       "16  0.000000    0.000000            0.000000   \n",
       "17  0.000000    0.000000            0.000000   \n",
       "18  0.000000    0.000000            0.000000   \n",
       "19  0.000000    0.000000            0.000000   \n",
       "\n",
       "    xxxmobilemovieclubcomnqjkgighjjgcbl        ye       you         ü  \n",
       "0                              0.000000  0.000000  0.000000  0.000000  \n",
       "1                              0.000000  0.000000  0.000000  0.000000  \n",
       "2                              0.000000  0.000000  0.000000  0.000000  \n",
       "3                              0.000000  0.000000  0.000000  0.000000  \n",
       "4                              0.000000  0.000000  0.000000  0.000000  \n",
       "5                              0.000000  0.000000  0.000000  0.000000  \n",
       "6                              0.000000  0.000000  0.000000  0.000000  \n",
       "7                              0.000000  0.000000  0.000000  0.000000  \n",
       "8                              0.000000  0.000000  0.000000  0.000000  \n",
       "9                              0.000000  0.000000  0.238188  0.000000  \n",
       "10                             0.265144  0.000000  0.000000  0.000000  \n",
       "11                             0.000000  0.000000  0.000000  0.000000  \n",
       "12                             0.000000  0.276948  0.000000  0.000000  \n",
       "13                             0.000000  0.000000  0.000000  0.000000  \n",
       "14                             0.000000  0.000000  0.000000  0.000000  \n",
       "15                             0.000000  0.000000  0.000000  0.000000  \n",
       "16                             0.000000  0.000000  0.000000  0.000000  \n",
       "17                             0.000000  0.000000  0.000000  0.333333  \n",
       "18                             0.000000  0.000000  0.000000  0.000000  \n",
       "19                             0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[20 rows x 212 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tfidf_df = pd.DataFrame(x_tfidf_sample.toarray())\n",
    "\n",
    "x_tfidf_df.columns = tfidf_vect_sample.get_feature_names()\n",
    "\n",
    "x_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Feature Engineering\n",
    "\n",
    "Feature engineerig is the process of creating new features or transoforming the existing features to get the most out of our data.\n",
    "\n",
    "In Feature Engineering we try to know what addition information would be helpful for our model to make accurate predictions.\n",
    "\n",
    "The main goal of Feature Engineering in our project is the create new features that would help the model to destinguish between spam and ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "data.columns = ['lable', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature for text message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "   body_len  \n",
       "0       128  \n",
       "1        49  \n",
       "2        62  \n",
       "3        28  \n",
       "4       135  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are using the approach assuming that Spam messages are longer than Ham\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature for % of text that is punctuation using the raw text as clean text does not have punctuation\n",
    "\n",
    "#### We have create 2 new Features 'Body Length' and \"Punctuation Percentage\"\n",
    "\n",
    "\n",
    "\n",
    "count = (1 for char is text if char in string.punctuation)\n",
    "\n",
    "'''\n",
    "The above code will return the character which is not in string.punctuation\n",
    "\n",
    "Now we have to flip it and cycling through each character to see if it is punctuation\n",
    "then instead of returning that character, we are just retruning 1.\n",
    "We want the sum of all of thos 1's, so we are implementing that in below code.\n",
    "\n",
    "We want the percent of the text message that are puncturation\n",
    "So we are dividing the count with the length of the text message, which is the number of characters in the text message.\n",
    "And we are replacing the 'x' with 'text' that we are using in the loop.\n",
    "\n",
    "what below return function tells us is to take the length of the text message\n",
    "and substract it with the number of spaces that you find in that text message.\n",
    "We are rounding it to 3 decimal places.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "   body_len  punct%  \n",
       "0       128     4.7  \n",
       "1        49     4.1  \n",
       "2        62     3.2  \n",
       "3        28     7.1  \n",
       "4       135     4.4  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def count_punct(text):\n",
    " \n",
    "\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "data.head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating new features using Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have created new two features, we want to exaluate those features\n",
    "# ...to see if they are providing any new value to the model, to distinguish Spam and Non-Spam\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Qw are using 2 histograms, first to determing the body length for Spam\n",
    "And we gonna use the second one to look at the body length of Non-Spam\n",
    "'''\n",
    "\n",
    "# 0 is the starting point, max 200 rows and 40 cut points.\n",
    "# So below code will return the 40evenly spaced numbers between 0 to 200\n",
    "# alpha parimeter is use for how dark we want our graph to be.\n",
    "# Normed = True this will normalize our graph to the same scale.\n",
    "\n",
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(data[data['lable'] == 'spam']['body_len'], bins, alpha=0.5, normed=True, label='spam')\n",
    "\n",
    "pyplot.hist(data[data['lable'] == 'ham']['body_len'], bins, alpha=0.5, normed=True, label='ham')\n",
    "\n",
    "pyplot.legend(loc = 'upper left')\n",
    "\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EVALUATING PUNCTUATION PERCENTAGE FEATURE, NO BINS HAVE MORE THAN 50% PUNCTUATION\n",
    "# From the Graph below we can see that Spam messages are longer than Ham messages.\n",
    "# HOWEVER THE ASSUMPTION THAT HAM CONTAINS LESS PUNCTUATIONS THAT SPAM DOESN'T SEEMS TO BE TRUE\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(data[data['lable'] == 'spam']['punct%'], bins, alpha=0.5, normed=True, label='spam')\n",
    "\n",
    "pyplot.hist(data[data['lable'] == 'ham']['punct%'], bins, alpha=0.5, normed=True, label='ham')\n",
    "\n",
    "pyplot.legend(loc = 'upper right')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineerign: Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "data.columns = ['lable', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Two new features (Body Length and Punctuation Percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lable  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "   body_len  punct%  \n",
       "0       128     4.7  \n",
       "1        49     4.1  \n",
       "2        62     3.2  \n",
       "3        28     7.1  \n",
       "4       135     4.4  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def count_punct(text):\n",
    "\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "'''When creating a lambda expression, you need to count the characters and ignore white spaces. \n",
    "What expression that helps you accomplish the task.\n",
    "\n",
    "Answer:  lambda x: len(x) - x.count(\" \")  '''\n",
    "\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Two new Features(Body Length and Punctuation Percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IN THIS SECTION WE WANT TO SEE THE FULL HISTOGRAM INSTEAD OF JUST CONCENTRATING ON HAMS AND SPAMS\n",
    "\n",
    "# In the graph below the right section is Spam and left section is Ham\n",
    "\n",
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(data['body_len'], bins)\n",
    "pyplot.title(\"Body Length Distribution\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(data['punct%'], bins)\n",
    "pyplot.title(\"Punctuation % Distribution\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Box-Cox power transformation\n",
    "\n",
    "Transformation: It is the process that alters each data point  in certain column in a systematic way, that makes it cleaner for the model to use.\n",
    "\n",
    "For eg: The transformation could be squaring each value in column or may be taking the square root of each value in the column.\n",
    "\n",
    "Box-Cox prower transformation:\n",
    "\n",
    "The base form of this type of transformation is y^x, where y is the value and in an individual cell and the x is the exponent of the power transformation we are applying.\n",
    "\n",
    "Transformation Process:\n",
    "\n",
    "1. Determine what range of exponents that we want to test\n",
    "\n",
    "2. Apply each transformation to each value of your chosen feature.\n",
    "\n",
    "3. Use some criteria to determine which of the transformations yiesd the best distribution. We can learn it from the various sources.\n",
    "\n",
    "\n",
    "**Base Form**:   $y^x $\n",
    "\n",
    "| X    | Base Form           |           Transformation               |\n",
    "|------|--------------------------|--------------------------|\n",
    "| -2   | $$ y ^ {-2} $$           | $$ \\frac{1}{y^2} $$      |\n",
    "| -1   | $$ y ^ {-1} $$           | $$ \\frac{1}{y} $$        |\n",
    "| -0.5 | $$ y ^ {\\frac{-1}{2}} $$ | $$ \\frac{1}{\\sqrt{y}} $$ |\n",
    "| 0    | $$ y^{0} $$              | $$ log(y) $$             |\n",
    "| 0.5  | $$ y ^ {\\frac{1}{2}}  $$ | $$ \\sqrt{y} $$           |\n",
    "| 1    | $$ y^{1} $$              | $$ y $$                  |\n",
    "| 2    | $$ y^{2} $$              | $$ y^2 $$                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The Plot title will change on every loop, when we apply format function to it\n",
    "# The i will represent the denominator in our exponent\n",
    "\n",
    "for i in [1, 2, 3, 4, 5]:\n",
    "    \n",
    "    pyplot.hist((data['punct%'])**(1/i), bins=40)\n",
    "    \n",
    "    pyplot.title(\"Transformation: 1/{}\".format((i)))\n",
    "    \n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "#  The graph in the left is at 0, which means there is no punctuation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Building Machine Learning Classifiers\n",
    "\n",
    "\n",
    "Types of Evaluation Matrics\n",
    "\n",
    "1. Accuracy\n",
    "\n",
    "2. Precision\n",
    "\n",
    "3. Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Machine Learning Classifiers: Building a basic Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8324</th>\n",
       "      <th>8325</th>\n",
       "      <th>8326</th>\n",
       "      <th>8327</th>\n",
       "      <th>8328</th>\n",
       "      <th>8329</th>\n",
       "      <th>8330</th>\n",
       "      <th>8331</th>\n",
       "      <th>8332</th>\n",
       "      <th>8333</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  8324  8325  \\\n",
       "0       128     4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49     4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62     3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28     7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135     4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8326  8327  8328  8329  8330  8331  8332  8333  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8336 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "\n",
    "data.columns = ['lable', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)* 100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "# USING THE STEMMBR IN THE BELOW LINE   \n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "x_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(x_tfidf.toarray())], axis= 1)\n",
    "x_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On cell 90,91,92, I was trying to remove the inconsistent numbers of samples: [5566, 5567]\n",
    "### which I was getting in the cell 95. But it was might because of lack of RAM Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nx_features.drop(data.index[::-3], inplace=True)\\n# df.drop(df.index[::-3], inplace=True)\\n\\n#data.drop(data.index[::-3], inplace=True)\\n\\nx_features.drop(data.index[::-3], inplace=True)\\nx_features.drop(data.index[::-3], inplace=True)\\nx_features.drop(data.index[::-3], inplace=True)\\n\\nx_features[::-1]\\n\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "x_features.drop(data.index[::-3], inplace=True)\n",
    "# df.drop(df.index[::-3], inplace=True)\n",
    "\n",
    "#data.drop(data.index[::-3], inplace=True)\n",
    "\n",
    "x_features.drop(data.index[::-3], inplace=True)\n",
    "x_features.drop(data.index[::-3], inplace=True)\n",
    "x_features.drop(data.index[::-3], inplace=True)\n",
    "\n",
    "x_features[::-1]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_features.drop(df.index[5566] , inplace=True)\n",
    "#x_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_features))\n",
    "print(len(data['body_len']))\n",
    "print(len(data['punct%']))\n",
    "\n",
    "print(len(data['body_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore RandomForestClassifier Attributes & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_estimator_type', '_get_param_names', '_make_estimator', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'decision_path', 'feature_importances_', 'fit', 'get_params', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params']\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(dir(RandomForestClassifier))\n",
    "print(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore RandomForestClassifier through Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96678636, 0.98294434, 0.96765499, 0.9541779 , 0.97304582])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By setting 'n_jobs = -1'  will help us to run our model faster, by creating several\n",
    "# ...decision trees in parallel\n",
    "\n",
    "# The below code if giving error as the values are not same.\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs = -1)\n",
    "k_fold = KFold(n_splits = 5)\n",
    "\n",
    "cross_val_score(rf, x_features, data['lable'], cv = k_fold, scoring='accuracy', n_jobs= -1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Machine Learning Classifiers: Random Forest on a holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Clean the Text\n",
    "\n",
    "1. First we are reading our Data\n",
    "\n",
    "2. Creating the new Features\n",
    "\n",
    "3. Cleaning that Data\n",
    "\n",
    "4. Then Vectorizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8324</th>\n",
       "      <th>8325</th>\n",
       "      <th>8326</th>\n",
       "      <th>8327</th>\n",
       "      <th>8328</th>\n",
       "      <th>8329</th>\n",
       "      <th>8330</th>\n",
       "      <th>8331</th>\n",
       "      <th>8332</th>\n",
       "      <th>8333</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  8324  8325  \\\n",
       "0       128     4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49     4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62     3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28     7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135     4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8326  8327  8328  8329  8330  8331  8332  8333  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8336 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# 1. Reading the Data\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "\n",
    "# 2. Creating new Features\n",
    "\n",
    "data.columns = ['lable', 'body_text']\n",
    "\n",
    "# 3. Cleaning the Data\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)* 100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "# USING THE STEMMBR IN THE BELOW LINE   \n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# 4. Vectorizing the Data\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "x_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(x_tfidf.toarray())], axis= 1)\n",
    "x_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Random Forest Classifier through Holdout Test Set\n",
    "\n",
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'train_test_split' will output Four Datasets, so we need to tell\n",
    "# ...what we want to store it as.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, data['lable'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing and Instantiating RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs= -1)\n",
    "\n",
    "rf_model = rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.08722083078883845, 'body_len'),\n",
       " (0.05218005346089037, 2016),\n",
       " (0.030080639648899225, 2243),\n",
       " (0.02500125690739171, 3348),\n",
       " (0.02219255133496166, 7574),\n",
       " (0.021058765634501375, 6962),\n",
       " (0.01890049231735121, 5006),\n",
       " (0.014715742573641992, 2383),\n",
       " (0.01422256419371824, 7244),\n",
       " (0.014089731176582916, 5942)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USING 'Feature_Importances' method offered by Random Forest\n",
    "\n",
    "sorted(zip(rf_model.feature_importances_, x_train.columns), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE CAN SEE FROM THE ABOVE RESULT THAT THE 'BODY_LEN' IS THE MOST IMPORTANT FEATURE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE ARE USING ONLY X METHOD FOR THE PREDICTION\n",
    "\n",
    "y_pred = rf_model.predict(x_test)\n",
    "precision,recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.604 / Accuracy: 0.9488330341113106\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To calculate Accuracy, we will set y_pred = y_test. So what that will do, is it will pair\n",
    "y_pred and y_test together and it will look row by row and see if they are equal to each other.\n",
    "It they are the same, then it will return the True and if they are not the same, then it will\n",
    "return the False. And if we put .sum in the end, then it will sum all the True\n",
    "'''\n",
    "\n",
    "\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                     round(recall, 3),\n",
    "                                                     round(y_pred == y_test).sum() / len(y_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above result we can see that Precision is 100% which means the spam is identified as spam 100% of the time\n",
    "\n",
    "#### The Recall is 62% which means that the only 62% of the messages goes into the Spam folder and rest went to the inbox\n",
    "\n",
    "#### Accuracy is 94% which means our model manage to identify Spam and Non-Spam 94% of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest model with Grid Search\n",
    "\n",
    "Grid Search: In Grid Search we exhaustively seach all parameter combinations in a given grid to determine the best model.\n",
    "\n",
    "Our last model was not eggressive enough, it was not capturing enough Spam emails. We can get better result by altering the Hyperparameter settings..?\n",
    "\n",
    "That's where we use the Grid-Search, it is basically a grid of Hyperparameter settings, and then exploring a model fit with each combination of those Hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8324</th>\n",
       "      <th>8325</th>\n",
       "      <th>8326</th>\n",
       "      <th>8327</th>\n",
       "      <th>8328</th>\n",
       "      <th>8329</th>\n",
       "      <th>8330</th>\n",
       "      <th>8331</th>\n",
       "      <th>8332</th>\n",
       "      <th>8333</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  8324  8325  \\\n",
       "0       128     4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49     4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62     3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28     7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135     4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8326  8327  8328  8329  8330  8331  8332  8333  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8336 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# 1. Reading the Data\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "\n",
    "# 2. Creating new Features\n",
    "\n",
    "data.columns = ['lable', 'body_text']\n",
    "\n",
    "# 3. Cleaning the Data\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)* 100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "# USING THE STEMMBR IN THE BELOW LINE   \n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# 4. Vectorizing the Data\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "x_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(x_tfidf.toarray())], axis= 1)\n",
    "x_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_features, data['lable'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs = -1)\n",
    "    rf_model = rf.fit(x_train, y_train)\n",
    "    y_pred = rf.fit(x_train, y_train)\n",
    "    y_pred= rf_model.predict(x_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ---- Precision: 1.0 / Recall: 0.534 / Accuracy: 0.945\n",
      "Est: 10 / Depth: 20 ---- Precision: 1.0 / Recall: 0.466 / Accuracy: 0.937\n",
      "Est: 10 / Depth: 30 ---- Precision: 1.0 / Recall: 0.435 / Accuracy: 0.934\n",
      "Est: 10 / Depth: None ---- Precision: 1.0 / Recall: 0.504 / Accuracy: 0.942\n",
      "Est: 50 / Depth: 10 ---- Precision: 1.0 / Recall: 0.527 / Accuracy: 0.944\n",
      "Est: 50 / Depth: 20 ---- Precision: 1.0 / Recall: 0.481 / Accuracy: 0.939\n",
      "Est: 50 / Depth: 30 ---- Precision: 1.0 / Recall: 0.511 / Accuracy: 0.943\n",
      "Est: 50 / Depth: None ---- Precision: 1.0 / Recall: 0.458 / Accuracy: 0.936\n",
      "Est: 100 / Depth: 10 ---- Precision: 1.0 / Recall: 0.511 / Accuracy: 0.943\n",
      "Est: 100 / Depth: 20 ---- Precision: 1.0 / Recall: 0.519 / Accuracy: 0.943\n",
      "Est: 100 / Depth: 30 ---- Precision: 1.0 / Recall: 0.473 / Accuracy: 0.938\n",
      "Est: 100 / Depth: None ---- Precision: 1.0 / Recall: 0.519 / Accuracy: 0.943\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Random Forest with GridSearchCV - Using K-Fold and Grid-search\n",
    "\n",
    "Grid-search: Exhaustively search all parameter combinations in a given grid to determine the best model.\n",
    "\n",
    "Cross-validation(K-Fold): Divide a dataset into k subsets and repeat the holdout method k times where a different subset is used as the holdout set in each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8324</th>\n",
       "      <th>8325</th>\n",
       "      <th>8326</th>\n",
       "      <th>8327</th>\n",
       "      <th>8328</th>\n",
       "      <th>8329</th>\n",
       "      <th>8330</th>\n",
       "      <th>8331</th>\n",
       "      <th>8332</th>\n",
       "      <th>8333</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8336 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%  0  1  2  3  4  5  6  7  ...  8324  8325  8326  8327  \\\n",
       "0       128     4.7  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "1        49     4.1  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "2        62     3.2  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "3        28     7.1  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "4       135     4.4  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "\n",
       "   8328  8329  8330  8331  8332  8333  \n",
       "0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 8336 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "# 1. Reading the Data\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep = '\\t')\n",
    "\n",
    "# 2. Creating new Features\n",
    "\n",
    "data.columns = ['lable', 'body_text']\n",
    "\n",
    "# 3. Cleaning the Data\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)* 100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "# USING THE STEMMBR IN THE BELOW LINE   \n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# 4. Vectorizing the Data\n",
    "# TF-IDF\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "x_tfidf_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(x_tfidf.toarray())], axis=1)\n",
    "\n",
    "\n",
    "# CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "x_count = count_vect.fit_transform(data['body_text'])\n",
    "x_count_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(x_count.toarray())], axis=1)\n",
    "\n",
    "x_count_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring parameter settings using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running code on TF-IDF\n",
    "\n",
    "Can not run because of not enought memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# param = parameters\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "\n",
    "# cv = cross validation folds = 5\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(x_tfidf_feat, data['lable'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running code on Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# param = parameters\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "# cv = cross validation folds = 5\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(x_count_feat, data['lable'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting and Random Forest\n",
    "\n",
    "The Gradient Boosting and Random Forest are both Ensmble Methods, Means they both combine several weak models and makes a strong model.\n",
    "\n",
    "Both works on Decision Trees as well.\n",
    "\n",
    "The differnece between Gradient Boosting and Random Forest is that Gradient Boosting uses Boosing and Random Forest uses Bagging.\n",
    "\n",
    "Random Forest(Bagging): \n",
    "- It samples randomly. \n",
    "- Each tree in random forest is indepandent which means we can run several trees parallelly, means at the same time. It was have 100 trees, we can run them at the same time. \n",
    "- It is easy to Tune, Harder to overfit and is fast.\n",
    "\n",
    "Gradient Boosting(Boosting): \n",
    "\n",
    "- It samples with an increased weight on the once that it got wrong previously. \n",
    "- It is iterative, which means it relies on the previous results of the trees, in order to apply a higher weight to the ones that previous tree got incorrect. \n",
    "- Boosting can't be parallelized and it takes much longer to train.\n",
    "- It is harder to Tune and easier to Overfit\n",
    "- The Gradient Boosting is powerful and better preforming if it is Tuned properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Machine Leanring Classifier: Exploring Gradient Boosting  with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "X_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring GradientBoostingClassifier Attributes & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print(dir(GradientBoostingClassifier), '\\n')\n",
    "print(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_GB(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        est, max_depth, lr, round(precision, 3), round(recall, 3), \n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: The loop below will run slowly and will consume too much memory as we are using Gradient Boosting not the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Gradient Boosting model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "X_tfidf_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_count = count_vect.fit_transform(data['body_text'])\n",
    "X_count_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_count.toarray())], axis=1)\n",
    "\n",
    "X_count_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring parameter settings using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running code on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "# In Gradient Boosting the n_jobs = -1 means that it will train model on different subsets and parameter\n",
    "# ...settings in parallel. It does not mean that it will train models in parallel because GB can't do that\n",
    "\n",
    "clf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "cv_fit = clf.fit(X_tfidf_feat, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running code on Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [50, 100, 150], \n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "cv_fit = clf.fit(X_count_feat, data['label'])\n",
    "pd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection: Model which is giving the best result\n",
    "\n",
    "#### Process\n",
    "\n",
    "1. Split the data into Training and Test set\n",
    "\n",
    "2. Train vectorizer on training set and use it to transform the test set.\n",
    "\n",
    "3. Fit the best Random Forest model and best Gradient Boolsting model on training se t and predict on test set.\n",
    "\n",
    "4. Thoroughly eveluate results of these two models to select best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting into Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], \n",
    "                                                    data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "\n",
    "# THE RESET INDEX FUNCTION WILL RESET THE INDEX TO THE NEW INDEX, WHICH IS CREATED BY THE NEW DATA-FRAME\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Evaluation Model (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result - Using the Time functiion to determine how long it took our model to compute\n",
    "\n",
    "We will mainly focus on Predict time, Precision and Recall. The reason is once these models are fit, we generally store them for the purpose of making predictions later on. They won't be ever re-fit or re-trained again until you decide that your current model needs to be replaced.\n",
    "\n",
    "So, that's why we care more about the Predict Time than the Fit Time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "# time.time() will take the start time, means when our model will start to compute\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "# pos_label = Positive Label\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), \n",
    "    round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), \n",
    "    round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
