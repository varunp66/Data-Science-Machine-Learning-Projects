{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Facenet model\n",
    "\n",
    "The CNN architecture used here is a variant of the inception architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8428c034dcec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnn4_small2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named model"
     ]
    }
   ],
   "source": [
    "from model import create_model\n",
    "\n",
    "nn4_small2 = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training aims to learn an embedding $f(x)$ of image $x$ such that the squared L2 distance between all faces of the same identity is small and the distance between a pair of faces from different identities is large. This can be achieved with a *triplet loss* $L$ that is minimized when the distance between an anchor image $x^a_i$ and a positive image $x^p_i$ (same identity) in embedding space is smaller than the distance between that anchor image and a negative image $x^n_i$ (different identity) by at least a margin $\\alpha$.\n",
    "\n",
    "$$L = \\sum^{m}_{i=1} \\large[ \\small {\\mid \\mid f(x_{i}^{a}) - f(x_{i}^{p})) \\mid \\mid_2^2} - {\\mid \\mid f(x_{i}^{a}) - f(x_{i}^{n})) \\mid \\mid_2^2} + \\alpha \\large ] \\small_+$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "s='jpg'\n",
    "a=\"jpeg\"    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        #print(i)\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "             if f.endswith(s) or f.endswith(a):\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata('Project')\n",
    "print(metadata.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from align import AlignDlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    #destRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img[...,::-1]\n",
    "    #return destRGB\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image \n",
    "jc_orig = load_image(metadata[89].image_path())\n",
    "\n",
    "# Detect face and return bounding box\n",
    "bb = alignment.getLargestFaceBoundingBox(jc_orig)\n",
    "\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "jc_aligned = alignment.align(96, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "# Show original image\n",
    "f = plt.figure(figsize=(20,20))\n",
    "f.add_subplot(131)\n",
    "\n",
    "#plt.subplots(figsize=(4, 4))\n",
    "plt.imshow(jc_orig)\n",
    "#f.savefig(\"example.jpg\")\n",
    "# Show original image with bounding box\n",
    "f.add_subplot(132)\n",
    "#subplots(figsize=(4, 4))\n",
    "plt.imshow(jc_orig)\n",
    "plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "#f.savefig(\"detect_face.jpg\")\n",
    "# Show aligned image\n",
    "f.add_subplot(133)\n",
    "#plt.subplots(figsize=(4, 4))\n",
    "plt.imshow(jc_aligned);\n",
    "#f.savefig(\"align_face.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_image(img):\n",
    "    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), \n",
    "                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding vectors can now be calculated by feeding the aligned and scaled images into the pre-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded = np.zeros((metadata.shape[0], 128))\n",
    "\n",
    "#print(metadata)\n",
    "k=0\n",
    "for i, m in enumerate(metadata):\n",
    "    \n",
    "        #print(i,m.image_path())\n",
    "        img = load_image(m.image_path())\n",
    "        \n",
    "        img = align_image(img)\n",
    "        # scale RGB values to interval [0,1]\n",
    "        img = (img / 255.).astype(np.float32)\n",
    "        # obtain embedding vector for image\n",
    "        embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify on a single triplet example that the squared L2 distance between its anchor-positive pair is smaller than the distance between its anchor-negative pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))\n",
    "\n",
    "def show_pair(idx1, idx2):\n",
    "    f=plt.figure(figsize=(10,5))\n",
    "    plt.suptitle(f'Distance = {distance(embedded[idx1], embedded[idx2]):.2f}')\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    plt.imshow(load_image(metadata[idx1].image_path()))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(load_image(metadata[idx2].image_path())); \n",
    " \n",
    "show_pair(0, 8)\n",
    "show_pair(0, 22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "identical = [] # 1 if same identity, 0 otherwise\n",
    "\n",
    "num = len(metadata)\n",
    "\n",
    "for i in range(num - 1):\n",
    "    for j in range(1, num):\n",
    "        distances.append(distance(embedded[i], embedded[j]))\n",
    "        identical.append(1 if metadata[i].name == metadata[j].name else 0)\n",
    "        \n",
    "distances = np.array(distances)\n",
    "identical = np.array(identical)\n",
    "\n",
    "thresholds = np.arange(0.3, 1.0, 0.01)\n",
    "\n",
    "f1_scores = [f1_score(identical, distances < t) for t in thresholds]\n",
    "acc_scores = [accuracy_score(identical, distances < t) for t in thresholds]\n",
    "\n",
    "opt_idx = np.argmax(f1_scores)\n",
    "# Threshold at maximal F1 score\n",
    "opt_tau = thresholds[opt_idx]\n",
    "# Accuracy at maximal F1 score\n",
    "opt_acc = accuracy_score(identical, distances < opt_tau)\n",
    "\n",
    "# Plot F1 score and accuracy as function of distance threshold\n",
    "plt.plot(thresholds, f1_scores, label='F1 score');\n",
    "plt.plot(thresholds, acc_scores, label='Accuracy');\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title(f'Accuracy at threshold {opt_tau:.2f} = {opt_acc:.3f}');\n",
    "plt.xlabel('Distance threshold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the distance between the two images of Jacques Chirac is smaller than the distance between an image of Jacques Chirac and an image of Gerhard SchrÃ¶der (0.30 < 1.12). But we still do not know what distance threshold $\\tau$ is the best boundary for making a decision between *same identity* and *different identity*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_pos = distances[identical == 1]\n",
    "dist_neg = distances[identical == 0]\n",
    "\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(dist_pos)\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title('Distances (pos. pairs)')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(dist_neg)\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title('Distances (neg. pairs)')\n",
    "plt.legend();\n",
    "fig.savefig(\"dist_histo.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "svc = SVC(kernel='linear', probability=True)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The KNN classifier achieves an accuracy of 92% on the test set, the SVM classifier 94%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training with all  traininig data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) \n",
    "\n",
    "#test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "#X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "#y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "svc = SVC(kernel='linear', probability=True)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "#acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed the dataset into 2D space for displaying identity clusters, [t-distributed Stochastic Neighbor Embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (t-SNE) is applied to the 128-dimensional embedding vectors. Except from a few outliers, identity clusters are well separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embedded)\n",
    "fig=plt.figure(figsize=(15,8))\n",
    "for i, t in enumerate(set(targets)):\n",
    "    idx = targets == t\n",
    "    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));\n",
    "fig.savefig(\"cluster.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition based on Hog face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from align import AlignDlib\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# Initialize the OpenFace face alignment utility\n",
    "\n",
    "start_time = time.clock()\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    #destRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img[...,::-1]\n",
    "    #return destRGB\n",
    "\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image of Jacques Chirac\n",
    "im_orig = load_image('t2.jpg')\n",
    "\n",
    "# Detect face and return bounding box\n",
    "bb = alignment.getAllFaceBoundingBoxes(im_orig)\n",
    "#print(bb)\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "im_aligned=[]\n",
    "for i in bb:\n",
    "      j = alignment.align(96, im_orig, i, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "      im_aligned.append(j)   \n",
    "\n",
    "# Show original image\n",
    "#plt.subplot(221)\n",
    "#plt.imshow(im_orig)\n",
    "\n",
    "fig=plt.figure(figsize=(15,15))\n",
    "plt.imshow(im_orig)\n",
    "# Show original image with bounding box\n",
    "#plt.subplot(222)\n",
    "#print(im_aligned)\n",
    "#plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "for i in bb:\n",
    "    \n",
    "    plt.gca().add_patch(patches.Rectangle((i.left(), i.top()), i.width(), i.height(), fill=False, color='red',linewidth=2))\n",
    "\n",
    "# Show aligned image\n",
    "#plt.subplot(133)\n",
    "#fig.savefig('time_hog_detection.jpg')\n",
    "#print(\"--- %s seconds ---\" % (time.clock() - start_time))\n",
    "for n in im_aligned:\n",
    "    n = (n / 255.).astype(np.float32)\n",
    "    embedded_t = nn4_small2_pretrained.predict(np.expand_dims(n, axis=0))[0]\n",
    "    #print(embedded)\n",
    "    embedded_t=embedded_t.reshape(1,-1)\n",
    "    #print( embedded_t)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    example_predic = knn.predict(embedded_t)\n",
    "    example_prob = svc.predict_proba(embedded_t)\n",
    "    #print(example_prob)\n",
    "    if np.any(example_prob>0.25):\n",
    "        example_i = encoder.inverse_transform(example_predic)[0]\n",
    "        plt.subplots(figsize=(4, 4))\n",
    "        plt.imshow(n)\n",
    "    \n",
    "        plt.title(f'Recognized as {example_i}')\n",
    "           \n",
    "    else:\n",
    "            plt.subplots(figsize=(4, 4))\n",
    "            plt.imshow(n)\n",
    "\n",
    "            plt.title('N/A')\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Face Recognition Based Attendance System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import pickle\n",
    "from sklearn import neighbors\n",
    "import time\n",
    "\n",
    "# This is a demo of running face recognition on live video from your webcam. It's a little more complicated than the\n",
    "# other example, but it includes some basic performance tweaks to make things run a lot faster:\n",
    "#   1. Process each video frame at 1/4 resolution (though still display it at full resolution)\n",
    "#   2. Only detect faces in every other frame of video.\n",
    "\n",
    "# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n",
    "# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n",
    "# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)    \n",
    "    \n",
    "\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "present=[]\n",
    "c=[]\n",
    "process_this_frame = True\n",
    "if __name__ == \"__main__\":\n",
    "    #knn_clf = train(\"knn_examples/train\")\n",
    "    knn_clf = pickle.load(open(\"knn_model.sav\", 'rb'))\n",
    "    \n",
    "    while True:\n",
    "        # Grab a single frame of video\n",
    "        ret, frame = video_capture.read()\n",
    "\n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "        # Only process every other frame of video to save time\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "            if (len(face_encodings)>0):\n",
    "                \n",
    "                closest_distances = knn_clf.kneighbors(face_encodings, n_neighbors=1)\n",
    "\n",
    "\n",
    "                is_recognized = [closest_distances[0][i][0] <= 0.5 for i in range(len(face_locations))]\n",
    "\n",
    "                face_names = []\n",
    "                for pred, loc, rec in zip(knn_clf.predict(face_encodings),face_locations, is_recognized):\n",
    "                     if rec:\n",
    "                         face_names.append(pred)\n",
    "                         if pred not in present: \n",
    "                                present.append(pred)\n",
    "                                c.append(time.strftime('%H:%M:%S', time.localtime()))\n",
    "                         \n",
    "                         \n",
    "                        \n",
    "                     else :\n",
    "                         face_names.append(\"unknown\")\n",
    "                 \n",
    "        \n",
    "\n",
    "        process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "        # Display the results\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            #print(name)\n",
    "            cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Hit 'q' on the keyboard to quit!\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "present\n",
    "t=[0,0,0,0,0,0,0,0,0,0]\n",
    "R=['101','102','103','104','105','106','107','108','109','110']\n",
    "n=[ 'Aditya','Aefaaz','Amogh','Ashutosh','Harsh','Karan', 'Nitish', 'Sarosh','Shivam','Shreyas' ]\n",
    "a=[0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(len(present)):\n",
    "    if present[i] in n: \n",
    "        a.pop(n.index(present[i]))\n",
    "        a.insert(n.index(present[i]),'P')\n",
    "        t.pop(n.index(present[i]))\n",
    "        t.insert(n.index(present[i]),c[i])\n",
    "for _ in range(len(a)):\n",
    "        if a[_]==0:\n",
    "            a.pop(_)\n",
    "            a.insert(_,\"-\")\n",
    "            t.pop(_)\n",
    "            t.insert(_,\"-\")\n",
    "data = {'Roll No.':R,'Name':n,'P/A':a,'Time':t}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df=df[['Roll No.','Name','P/A','Time']]\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def hover(hover_color=\"#ffff99\"):\n",
    "    return dict(selector=\"tr:hover\",\n",
    "                props=[(\"background-color\", \"%s\" % hover_color)])\n",
    "\n",
    "styles = [\n",
    "    hover(),\n",
    "    dict(selector=\"th\", props=[(\"font-size\", \"150%\"),\n",
    "                               (\"text-align\", \"center\")]),\n",
    "    dict(selector=\"caption\", props=[(\"caption-side\", \"center\")])\n",
    "]\n",
    "def highlight(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    z=[]\n",
    "    for i in range(len(df.index)):\n",
    "         if df.iloc[i,2]=='P':\n",
    "                z.append(i)\n",
    "         else:\n",
    "                z.append(-1)     \n",
    "    \n",
    "    return ['background-color: greenyellow' if i==z[i] else ''for i in range(len(z))]\n",
    "html = (df.style.set_table_styles(styles)\n",
    "          .set_caption(\"Attendence Sheet.\").set_properties(**{'font-size':'11pt'})\n",
    "           .apply(highlight))\n",
    "\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "1. Deep face recognition with Keras, Dlib and OpenCV http://krasserm.github.io/2018/02/07/deep-face-recognition/\n",
    "2. https://github.com/ageitgey/face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
